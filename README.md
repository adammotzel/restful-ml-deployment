# ML Deployment API

This repository demonstrates how to serve a trained Machine Learning model as a basic REST API endpoint using **FastAPI**. The following steps are covered:

1. **Train a Logistic Regression Model**: See `notebooks/train.ipynb` for a simple example of model fitting. (Note: The primary focus of this project is deployment, so model training is kept minimal.)
2. **Create a Model Wrapper**: A wrapper class is set up in `src/model.py` to customize model behavior for deployment.
3. **Set Up Model Schemas**: API request and response schemas are defined using `pydantic` in `src/schemas.py`.
4. **Data Collection and Logging**: Set up logging and data capture clients in `src/clients.py` to handle input/output capture and event logging.
5. **FastAPI App for Model Deployment**: The app is created in `src/app.py`, which exposes an `/inference` endpoint and includes basic authentication.
6. **Create API Credentials**: Set up a basic set of API credentials in `scripts/creds.sh` and write them into a `.env` file.

The project directory structure is as follows:

```bash
├── data/
   ├── logs/
      └── .log             # files written by the src.clients.LoggingClient class
   └── monitoring/
      └── .csv             # files written by the src.clients.DataCollectionClient class
├── docs/
   └── api_specs.yaml      # api specs generated by src/openapi.py
├── models/
   └── model.pkl           # where notebooks/train.ipynb saves the fitted model pkl file
├── notebooks/
   └── train.ipynb         # trains and saves a Logistic Regression model to models/model.pkl
├── scripts/
   └── creds.sh            # generates a random set of auth credentials for the app and writes them to .env
├── src
   ├── __init__.py
   ├── app.py              # contains the FastAPI app code
   ├── clients.py          # defines the DataCollectionClient and LoggingClient classes used in the app
   ├── model.py            # defines the ModelWrapper class used in the app
   ├── openapi.py          # generates the docs/api_specs.yaml file
   ├── schemas.py          # defines the schemas/classes for endpoint inputs and outputs
   └── app_setup.py        # instantiates / initializes objects used in app.py
└── tests/
   ├── __init__.py
   └── test_app.py         # tests the basic functionality of the app
```


## Setup

To get started, follow these steps:

1. **Clone the Repository**:

   ```bash
   git clone https://github.com/adammotzel/ml-api-deployment
   ```

2. **Create a Virtual Environment**:

   I developed this project using Python `3.12.8`.

   ```bash
   python -m venv venv
   ```


3. **Activate the Virtual Environment**:

   - On Windows:

     ```bash
     venv\Scripts\activate
     ```

   - On macOS/Linux:

     ```bash
     source venv/bin/activate
     ```

4. **Install Dependencies**:

   ```bash
   pip install -r requirements.txt
   ```

5. **Set the Python Path**:

   Before running the app, ensure project root is added to the `PYTHONPATH`. From the project root, run:

   ```bash
   export PYTHONPATH=$PYTHONPATH:$(pwd)
   ```

6. **Create API Credentials**: 
   
   Set up a basic set of API credentials.
   
   ```bash
   source scripts/creds.sh
   ```
   
   This scripts generates a set of credentials required to access the app and writes them into a `.env` file.


## App Code

The main application code is located in the `src` directory.

### Custom Python Classes

The app leverages a few custom classes designed to support its functionality:

1. **`src.model.ModelWrapper`**  
   This class wraps the fitted model object to customize its functionality for the app. In this example, the model expects `numpy` arrays as input, which are not JSON serializable. The `ModelWrapper` class handles the necessary conversions to ensure that the model can interact with the app, making it compatible with the deployment environment.

2. **`src.clients.DataCollectionClient`**  
   This class is responsible for collecting input data and model predictions. It writes this information to CSV files in the `data/monitoring/` directory. The collected data is useful for model monitoring and performance analysis.

   In `app.py`, data is collected by the `DataCaptureClient` using a `ThreadPoolExecutor` instance with a single worker. We use a single worker to force all data collection operations to occur within a single, separate thread, avoiding concurrency issues when writing data to disk. NOTE: Using a single worker probably isn't feasible for large applications; but for this demonstration it is sufficient.

3. **`src.clients.LoggingClient`**  
   The `LoggingClient` class abstracts the logging configuration, providing a simple interface for adding logging statements throughout the application code. This helps ensure consistent logging practices. 
   
   A `FileHandler` and (optionally) a `StreamHandler` are added to the logger. Using the `FileHandler`, the logger will stream logs to `.log` files in `data/logs`. The `StreamHandler` streams logs to stderr, which is useful during development and testing.


## Running the App

By default, the app will not run out-of-the-box after cloning the repository because the fitted model object is missing from the `models` directory. To resolve this:

1. **Train the Model**: Execute the entire `notebooks/train.ipynb` notebook. This will train and save a Logistic Regression model as a `pickle` file (`models/model.pkl`).

2. **Run the App**: Once the `model.pkl` file is present, start the app by running the `run_app.py` script.

   You can serve the app however you like. By default, it is served locally (when running `run_app.py`), but you can also expose the app to other devices in a private network:

      - First ensure your machine is connected to the private, trusted network, like your home Wi-Fi. You can make your network "trusted" via your machine's network settings.
      - Set the `SERVER_IP` environment variable (and optionally `SERVER_PORT` for a custom port) to the private IP address assigned to your machine by your private network's router/DHCP server:

         ```bash
         export SERVER_IP=<your-private-ip>
         export SERVER_PORT=<desired-port>
         ```

      - Create an inbound firewall rule to allow incoming traffic to your app's port (e.g., `8000`). This may not be required for all machines.
      - Run the `run_app.py` script; the app should now be accessible to other devices in your private network.

      Serving the app on your private network helps simulate a 'production' environment without exposing the app to the public internet. Serving the app to the public internet is out-of-scope for this project.


## Running the App Using Docker

In the project root directory, a `Dockerfile` is included to containerize the app. Instead of using `venv`, you can build the Docker image and run the container directly:

```bash
docker build -t fastapi-app .
```
```bash
docker container run --name fastapi-container -p 8000:8000 fastapi-app
```

**A few important notes about the Docker approach:**

1. Ensure your Docker daemon is running before building the image.
2. The Dockerfile sets the `SERVER_IP` env variable to `0.0.0.0` to expose the app outside of the Docker container. The app can be reached using `localhost`, `127.0.0.1`, or your machine's private IP (i.e., other *in-network* devices can access the app).
3. By default, the `SERVER_PORT` env variable is set to `8000`. If you want to change the port, you can edit the Dockerfile.
4. When running the container, you need to map a port on your host machine to the port exposed in the Docker container. In this example, both are set to port `8000`. This is accomplished via `-p 8000:8000` in the `run` command. 
5. You still need to execute the `/notebooks/train.ipynb` notebook to fit and save the model object before running the app.
6. The app logs and captured request data/predictions are not stored outside of the container. If you want to store this data you can bind mount the `logs/` and `data/` directories when running the container. However, because the logging client uses a `StreamHandler`, you can still view the app logs in the container logs. 


## API Documentation

The app automatically generates OpenAPI documentation via **Swagger UI**. To view the API documentation, hit the `/docs` endpoint:

```
http://<server>:<port>/docs
```


### OpenAPI Specification

You can generate your own OpenAPI spec in YAML format by running the following script:

```bash
python -m src.openapi
```

The generated OpenAPI spec will be saved to `docs/api_specs.yaml`.


## Testing

To run a simple set of tests for the app, execute the `tests/test_app.py` script:

```bash
python -m tests.test_app
```

This will run the test suite and validate the core functionality of the app. Ensure the app is running before executing the script.
